{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ingo\\Desktop\\Code Stuff\\mae\\mae\\.mae-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "\n",
    "# add classification head\n",
    "vit.head = torch.nn.Linear(in_features=768, out_features=1000, bias=True)\n",
    "\n",
    "torch.save(vit.state_dict(), '../mae_timm_vit_base.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('../mae_finetuned_vit_base.pth', map_location='cpu')\n",
    "new_state_dict = {}\n",
    "for key, value in checkpoint['model'].items():\n",
    "    # Rename keys to match the model's expected keys\n",
    "    new_key = key.replace('fc_norm', 'norm')  # Adjust based on the specific mismatch\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "vit.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ingo\\Desktop\\Code Stuff\\mae\\mae\\.mae-env\\Lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 1000]                 152,064\n",
       "├─PatchEmbed: 1-1                        [1, 196, 768]             --\n",
       "│    └─Conv2d: 2-1                       [1, 768, 14, 14]          590,592\n",
       "│    └─Identity: 2-2                     [1, 196, 768]             --\n",
       "├─Dropout: 1-2                           [1, 197, 768]             --\n",
       "├─Identity: 1-3                          [1, 197, 768]             --\n",
       "├─Identity: 1-4                          [1, 197, 768]             --\n",
       "├─Sequential: 1-5                        [1, 197, 768]             --\n",
       "│    └─Block: 2-3                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-2               [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-3                [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-4                [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-6                     [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-7                [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-8                [1, 197, 768]             --\n",
       "│    └─Block: 2-4                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-10              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-11               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-12               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-14                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-15               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-16               [1, 197, 768]             --\n",
       "│    └─Block: 2-5                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-18              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-19               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-20               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-22                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-23               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-24               [1, 197, 768]             --\n",
       "│    └─Block: 2-6                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-26              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-27               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-28               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-30                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-31               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-32               [1, 197, 768]             --\n",
       "│    └─Block: 2-7                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-34              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-35               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-36               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-38                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-39               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-40               [1, 197, 768]             --\n",
       "│    └─Block: 2-8                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-42              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-43               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-44               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-46                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-47               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-48               [1, 197, 768]             --\n",
       "│    └─Block: 2-9                        [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-50              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-51               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-52               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-54                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-55               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-56               [1, 197, 768]             --\n",
       "│    └─Block: 2-10                       [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-58              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-59               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-60               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-62                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-63               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-64               [1, 197, 768]             --\n",
       "│    └─Block: 2-11                       [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-66              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-67               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-68               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-70                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-71               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-72               [1, 197, 768]             --\n",
       "│    └─Block: 2-12                       [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-74              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-75               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-76               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-78                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-79               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-80               [1, 197, 768]             --\n",
       "│    └─Block: 2-13                       [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-82              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-83               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-84               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-86                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-87               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-88               [1, 197, 768]             --\n",
       "│    └─Block: 2-14                       [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 197, 768]             1,536\n",
       "│    │    └─Attention: 3-90              [1, 197, 768]             2,362,368\n",
       "│    │    └─Identity: 3-91               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-92               [1, 197, 768]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 197, 768]             1,536\n",
       "│    │    └─Mlp: 3-94                    [1, 197, 768]             4,722,432\n",
       "│    │    └─Identity: 3-95               [1, 197, 768]             --\n",
       "│    │    └─Identity: 3-96               [1, 197, 768]             --\n",
       "├─LayerNorm: 1-6                         [1, 197, 768]             1,536\n",
       "├─Identity: 1-7                          [1, 768]                  --\n",
       "├─Dropout: 1-8                           [1, 768]                  --\n",
       "├─Linear: 1-9                            [1, 1000]                 769,000\n",
       "==========================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 201.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 162.19\n",
       "Params size (MB): 345.66\n",
       "Estimated Total Size (MB): 508.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(vit, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
